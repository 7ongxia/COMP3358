{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FSolGLg7_l4",
        "outputId": "eb3daa21-c38f-4bc7-85ce-c2a74ef8105d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=eb1d370896aefb82bada68680b80a9df9bcb4e13536bc663f59d829e7b34b371\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2**"
      ],
      "metadata": {
        "id": "6A-n6ZEbMMwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"app\").getOrCreate()\n",
        "\n",
        "# The preprocessed output from task 1 is converted into csv file for ease of processing in following tasks.\n",
        "df = spark.read.csv(\"search_data.csv\",header=True, sep='\\t')\n",
        "df.show()\n",
        "\n",
        "# Convert to RDD\n",
        "t2_tokens = df.rdd.flatMap(lambda row: row[2].split(\".\"))\n",
        "t2_pairs = t2_tokens.map(lambda token: (token, 1))\n",
        "\n",
        "# Reduce by key and count number of tokens\n",
        "t2_counts = t2_pairs.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Sort in descending order and extract top 10\n",
        "t2_top10 = t2_counts.sortBy(lambda x: x[1], ascending=False).take(10)\n",
        "\n",
        "for token, count in t2_top10:\n",
        "    print(f\"{token}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phNFMkDx8FFJ",
        "outputId": "b86ebbf4-726e-4705-a21a-eb940f9eafa5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+--------------------+\n",
            "|queryTime|  userId|            clickUrl|\n",
            "+---------+--------+--------------------+\n",
            "|  0:00:00|2.98e+15|  download.it.com.cn|\n",
            "|  0:00:00|7.59e+15|       news.21cn.com|\n",
            "|  0:00:00|5.23e+15|     www.greatoo.com|\n",
            "|  0:00:00|6.14e+15|       www.jd-cd.com|\n",
            "|  0:00:00|8.56e+15|       www.big38.net|\n",
            "|  0:00:00|2.39e+16|  www.chinabaike.com|\n",
            "|  0:00:00|1.80e+15|        www.6wei.net|\n",
            "|  0:00:00|7.18e+14|    www.shanziba.com|\n",
            "|  0:00:00|4.14e+16|       bbs.gouzai.cn|\n",
            "|  0:00:00|9.98e+15|     ks.cn.yahoo.com|\n",
            "|  0:00:00|2.16e+16|  www.fotolog.com.cn|\n",
            "|  0:00:00|7.42e+15|     ks.cn.yahoo.com|\n",
            "|  0:00:00|6.17e+14|    topic.bindou.com|\n",
            "|  0:00:00|3.93e+15|     ks.cn.yahoo.com|\n",
            "|  0:00:00|8.24e+15|shwamlys.blog.soh...|\n",
            "|  0:00:00|8.25e+15|  download.it168.com|\n",
            "|  0:00:00|6.24e+15|   www.songtaste.com|\n",
            "|  0:00:00|6.55e+15|   product.it168.com|\n",
            "|  0:00:00|2.35e+15|    pic.news.mop.com|\n",
            "|  0:00:00|6.48e+15|       www.1000dy.cn|\n",
            "+---------+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "com: 7935\n",
            "www: 4184\n",
            "cn: 2362\n",
            "baidu: 779\n",
            "news: 641\n",
            "net: 603\n",
            "zhidao: 530\n",
            "sina: 501\n",
            "bbs: 496\n",
            "sohu: 416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, expr"
      ],
      "metadata": {
        "id": "5jW_ojuwH8IY"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 3**"
      ],
      "metadata": {
        "id": "ZZWfl8bnNCUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_time = df.select(\"queryTime\")\n",
        "df_time.show()\n",
        "\n",
        "t3_tokens = df_time.withColumn(\"last_two_chars\", expr(\"substr(queryTime, -2)\")).select(\"last_two_chars\")\n",
        "\n",
        "t3_pairs = t3_tokens.rdd.map(lambda token: (token, 1))\n",
        "\n",
        "# Reduce by key and count number of token\n",
        "t3_counts = t3_pairs.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Sort the result in descending order of the count\n",
        "t3_top10 = t3_counts.sortBy(lambda x: x[1], ascending=False).take(10)\n",
        "\n",
        "# Print the result\n",
        "for token, count in t3_top10:\n",
        "    print(f\"{token}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpThQtpb9TJu",
        "outputId": "b43f7e3d-6c4a-4b6b-fe80-228f99b7790c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|queryTime|\n",
            "+---------+\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "|  0:00:00|\n",
            "+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Row(last_two_chars='00'): 201\n",
            "Row(last_two_chars='35'): 194\n",
            "Row(last_two_chars='23'): 192\n",
            "Row(last_two_chars='07'): 188\n",
            "Row(last_two_chars='31'): 187\n",
            "Row(last_two_chars='13'): 186\n",
            "Row(last_two_chars='19'): 184\n",
            "Row(last_two_chars='06'): 183\n",
            "Row(last_two_chars='09'): 180\n",
            "Row(last_two_chars='14'): 180\n"
          ]
        }
      ]
    }
  ]
}